{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f74525d2-9aba-4a56-bb56-18522f28adb1",
      "metadata": {
        "tags": [],
        "id": "f74525d2-9aba-4a56-bb56-18522f28adb1"
      },
      "source": [
        "#AISV_801 Natural Language Processing #\n",
        "\n",
        "## Instructor Joseph Meyer ##\n",
        "\n",
        "Lab4: Explore Word2vec\n",
        "\n",
        "Bobby Wen\n",
        "\n",
        "Dataset: poem_sentiment\n",
        "\n",
        "Exploration\n",
        "\n",
        "    Choose a word, find the n most similar words\n",
        "    \n",
        "    Code the famous king - man = queen\n",
        "\n",
        "Download Poem_sentiment\n",
        "\n",
        "    Create a function to calculate the average word2vec of a given string"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9485ccda-20cf-4e32-a50d-7152916ff28d",
      "metadata": {
        "id": "9485ccda-20cf-4e32-a50d-7152916ff28d"
      },
      "source": [
        "### Load required libraries ###"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzZFek6Qu32l",
        "outputId": "1554d583-3e33-471e-cd59-1f9ee8daed9f"
      },
      "id": "MzZFek6Qu32l",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d752d8-19ba-4fde-a647-c3ad9f372c3e",
      "metadata": {
        "id": "27d752d8-19ba-4fde-a647-c3ad9f372c3e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce6b528-762f-4119-ab98-0f52dbd4f647",
      "metadata": {
        "id": "3ce6b528-762f-4119-ab98-0f52dbd4f647"
      },
      "source": [
        "### load and check dataset ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2168b4dc-bcd9-4bc4-8878-ebb79b722e52",
      "metadata": {
        "id": "2168b4dc-bcd9-4bc4-8878-ebb79b722e52"
      },
      "outputs": [],
      "source": [
        "# #Load poem_sentiment dataset\n",
        "dataset = load_dataset(\"poem_sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b63fa5-010f-47a7-b782-cb142b592b73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37b63fa5-010f-47a7-b782-cb142b592b73",
        "outputId": "b169c5f1-384a-41ff-cf7f-4453c10a622c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'verse_text', 'label'],\n",
            "        num_rows: 892\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'verse_text', 'label'],\n",
            "        num_rows: 105\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'verse_text', 'label'],\n",
            "        num_rows: 104\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "### Check Dataset\n",
        "print (dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfa86491-0749-4c03-aaf2-ad9ce96862e7",
      "metadata": {
        "id": "bfa86491-0749-4c03-aaf2-ad9ce96862e7"
      },
      "outputs": [],
      "source": [
        "### train test split on just data is not needed, by could be used if needed\n",
        "# Using the train_test_split function, we are able to create training and testing data set from a single data set\n",
        "#X_train, X_test, y_train, y_test = train_test_split(dataset[\"train\"]['verse_text'], dataset[\"train\"]['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e8208ca-9e54-453c-89c1-aaa6a48cc966",
      "metadata": {
        "id": "0e8208ca-9e54-453c-89c1-aaa6a48cc966"
      },
      "outputs": [],
      "source": [
        "### Split the training and testing datasets as intended by the dataset\n",
        "X_train, y_train = dataset[\"train\"]['verse_text'], dataset[\"train\"]['label']\n",
        "X_test, y_test = dataset[\"test\"]['verse_text'], dataset[\"test\"]['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68177766-f596-4393-a8e1-70c3df4c7366",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68177766-f596-4393-a8e1-70c3df4c7366",
        "outputId": "e66b211d-cfb9-4e53-9b2e-ebb001796594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the head that lay against your knees\n",
            "892 Records\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "print (random.choice(X_train))\n",
        "print (len(X_train), \"Records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f2f3ddc-edd8-4c58-9262-230d50895ddd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f2f3ddc-edd8-4c58-9262-230d50895ddd",
        "outputId": "5c8efee4-8943-4090-8cb2-1b83eb71a67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "892 Records\n"
          ]
        }
      ],
      "source": [
        "print (random.choice(y_train))\n",
        "print (len(y_train), \"Records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac3e396b-ba8e-4700-b3cf-bff28bd47881",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac3e396b-ba8e-4700-b3cf-bff28bd47881",
        "outputId": "4087a467-5554-4959-e692-91eaa065b04a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "indignantly i hurled the cry:\n",
            "104 Records\n"
          ]
        }
      ],
      "source": [
        "print (random.choice(X_test))\n",
        "print (len(X_test), \"Records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0120c2a-47d9-4818-9745-74cd19a3189e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0120c2a-47d9-4818-9745-74cd19a3189e",
        "outputId": "42209a01-f845-4c3d-9850-2f8a9272aebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "104 Records\n"
          ]
        }
      ],
      "source": [
        "print (random.choice(y_test))\n",
        "print (len(y_test), \"Records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbf02bec-d6bd-4f19-bb90-4b51e6aa37b1",
      "metadata": {
        "id": "cbf02bec-d6bd-4f19-bb90-4b51e6aa37b1"
      },
      "outputs": [],
      "source": [
        "###  Class reference version\n",
        "\n",
        "# import gensim\n",
        "\n",
        "# def load_word2vec_model(model_path):\n",
        "#     # Load the Word2Vec model\n",
        "#     model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
        "#     return model\n",
        "\n",
        "# def find_similar_words(word, model, topn=5):\n",
        "#     # Find words similar to the given word\n",
        "#     similar_words = model.most_similar(word, topn=topn)\n",
        "#     return similar_words\n",
        "\n",
        "# def word_vector(word, model): # Get the vector representation of a word\n",
        "#     return model[word]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d98055-9ef9-4db2-909c-1e0b2db46427",
      "metadata": {
        "id": "93d98055-9ef9-4db2-909c-1e0b2db46427"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ce66c39-1f18-49ba-8406-ad887d53bd74",
      "metadata": {
        "tags": [],
        "id": "8ce66c39-1f18-49ba-8406-ad887d53bd74"
      },
      "source": [
        "### Gensim - “Generate Similar” ###\n",
        "\n",
        "Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community. It provides access to Word2Vec and other word embedding algorithms for training, and it also allows pre-trained word embeddings that can be downloaded from the internet to be loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fc54783-dad2-4afe-b0a8-304b39364a13",
      "metadata": {
        "tags": [],
        "id": "4fc54783-dad2-4afe-b0a8-304b39364a13"
      },
      "source": [
        "Why Gensim?\n",
        "Super fast\n",
        "The fastest library for training of vector embeddings – Python or otherwise. The core algorithms in Gensim use battle-hardened, highly optimized & parallelized C routines.\n",
        "\n",
        "** Data Streaming **\n",
        "Gensim can process arbitrarily large corpora, using data-streamed algorithms. There are no \"dataset must fit in RAM\" limitations.\n",
        "\n",
        "** Platform independent **\n",
        "Gensim runs on Linux, Windows and OS X, as well as any other platform that supports Python and NumPy.\n",
        "\n",
        "** Proven **\n",
        "With thousands of companies using Gensim every day, over 2600 academic citations and 1M downloads per week, Gensim is one of the most mature ML libraries.\n",
        "\n",
        "** Open source **\n",
        "All Gensim source code is hosted on Github under the GNU LGPL license, maintained by its open source community. For commercial arrangements, see Business Support.\n",
        "\n",
        "** Ready-to-use models and corpora **\n",
        "The Gensim community also publishes pretrained models for specific domains like legal or health, via the Gensim-data project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c6f52f1-c550-4ad1-a53e-739e5300bc6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c6f52f1-c550-4ad1-a53e-739e5300bc6e",
        "outputId": "bd06d712-08c7-4382-ec77-c63f84961aeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader\n",
        "# Show all available models in gensim-data\n",
        "print(list(gensim.downloader.info()['models']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c25a6066-88a1-4ea8-a75a-9fe2fcd78b79",
      "metadata": {
        "id": "c25a6066-88a1-4ea8-a75a-9fe2fcd78b79"
      },
      "source": [
        "** Models **\n",
        "\n",
        "\n",
        "name\tnum vectors \tfile size\tbase dataset\tdescription\n",
        "\n",
        "\n",
        "conceptnet-numberbatch-17-06-300\t1917247\t1168 MB\tConceptNet, word2vec, GloVe, and OpenSubtitles 2016\tConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.\n",
        "\n",
        "\n",
        "fasttext-wiki-news-subwords-300\t999999\t958 MB\tWikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)\t1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\n",
        "\n",
        "\n",
        "glove-twitter-100\t1193514\t387 MB\tTwitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\tPre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\n",
        "\n",
        "glove-twitter-200\t1193514\t758 MB\tTwitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\tPre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
        "\n",
        "glove-twitter-25\t1193514\t104 MB\tTwitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\tPre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
        "\n",
        "glove-twitter-50\t1193514\t199 MB\tTwitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\tPre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\n",
        "\n",
        "glove-wiki-gigaword-100\t400000\t128 MB\tWikipedia 2014 + Gigaword 5 (6B tokens, uncased)\tPre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
        "\n",
        "glove-wiki-gigaword-200\t400000\t252 MB\tWikipedia 2014 + Gigaword 5 (6B tokens, uncased)\tPre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
        "\n",
        "glove-wiki-gigaword-300\t400000\t376 MB\tWikipedia 2014 + Gigaword 5 (6B tokens, uncased)\tPre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
        "\n",
        "glove-wiki-gigaword-50\t400000\t65 MB\tWikipedia 2014 + Gigaword 5 (6B tokens, uncased)\tPre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
        "\n",
        "word2vec-google-news-300\t3000000\t1662 MB\tGoogle News (about 100 billion words)\tPre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\n",
        "\n",
        "\n",
        "\n",
        "word2vec-ruscorpora-300\t184973\t198 MB\tRussian National Corpus (about 250M words)\tWord2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54f3d5b3-c64b-402b-9c11-9b16c3c53c70",
      "metadata": {
        "id": "54f3d5b3-c64b-402b-9c11-9b16c3c53c70"
      },
      "source": [
        "GloVe stands for Global Vectors for Word Representation.\n",
        "\n",
        "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2856eab2-d057-453e-9824-13912061bf76",
      "metadata": {
        "id": "2856eab2-d057-453e-9824-13912061bf76"
      },
      "source": [
        "I am loading a pre-trained models to not have to train our dataset.\n",
        "\n",
        "A small data set glove-twitter-25 was used initially to test code.  Later larger data sets were tested.  Finally the largest dataset is used to test the famous King - man prediction.\n",
        "\n",
        "We use the pre-train model to find words similar to 'king'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99113ab-ad92-4aa0-a3ce-95f0645eaa03",
      "metadata": {
        "id": "d99113ab-ad92-4aa0-a3ce-95f0645eaa03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd26a3ee-f752-44b9-9f89-b798679ab91d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# Download the \"glove-twitter-25\" embeddings\n",
        "#w2v_model = gensim.downloader.load('glove-twitter-25')   ###  smallest data set to test code\n",
        "#w2v_model = gensim.downloader.load('glove-wiki-gigaword-300')   ### Larger pre-trained model to test accuracy\n",
        "w2v_model = gensim.downloader.load('fasttext-wiki-news-subwords-300')   ### Larger pre-trained model to maximize accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8afb195-0653-4595-9134-775760ff3c01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8afb195-0653-4595-9134-775760ff3c01",
        "outputId": "c488cd6e-8002-4011-96d8-7b9ef49912a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king-', 0.7838029861450195),\n",
              " ('boy-king', 0.7704817652702332),\n",
              " ('queen', 0.7704246640205383),\n",
              " ('prince', 0.7700967192649841),\n",
              " ('kings', 0.7668929696083069)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Use the downloaded vectors as usual:\n",
        "w2v_model.most_similar('king', topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db4b8c4f-bd6a-4d70-80f0-eb0ac1d61ce9",
      "metadata": {
        "id": "db4b8c4f-bd6a-4d70-80f0-eb0ac1d61ce9"
      },
      "source": [
        "Code the famous king - man = queen test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27c9da7d-43c5-4816-b5a2-6d38358dd0cc",
      "metadata": {
        "id": "27c9da7d-43c5-4816-b5a2-6d38358dd0cc"
      },
      "outputs": [],
      "source": [
        "vec = w2v_model['king'] - w2v_model['man']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82dd77eb-fdb5-4c4f-861b-341c921700fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82dd77eb-fdb5-4c4f-861b-341c921700fb",
        "outputId": "ee529593-2503-4dd5-b18d-9fc4349f46b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.4447827935218811),\n",
              " ('kings', 0.36625999212265015),\n",
              " ('Cináed', 0.36057743430137634),\n",
              " ('co-king', 0.35103851556777954),\n",
              " ('Wuyue', 0.3504401445388794)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "w2v_model.similar_by_vector(vec, topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbce63bd-503e-4aff-ad3d-5fc600c3ee4f",
      "metadata": {
        "id": "cbce63bd-503e-4aff-ad3d-5fc600c3ee4f"
      },
      "outputs": [],
      "source": [
        "vec = w2v_model['king'] - w2v_model['man'] + w2v_model['woman']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a847206-290d-4fbe-98bb-17ad5ef8aede",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a847206-290d-4fbe-98bb-17ad5ef8aede",
        "outputId": "618fbbfb-3e2e-4bf5-fb02-c6743e5c34af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.8185327053070068),\n",
              " ('queen', 0.7570335865020752),\n",
              " ('queen-mother', 0.6896809339523315),\n",
              " ('king-', 0.6787333488464355),\n",
              " ('queen-consort', 0.6555202603340149)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "w2v_model.similar_by_vector(vec, topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e40c1a-9fe3-4b65-a5ea-e7b03bc35d56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29e40c1a-9fe3-4b65-a5ea-e7b03bc35d56",
        "outputId": "24752386-5556-4987-b458-f2c65f813377"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.8185327053070068),\n",
              " ('queen', 0.7570335865020752),\n",
              " ('queen-mother', 0.6896809339523315),\n",
              " ('king-', 0.6787333488464355),\n",
              " ('queen-consort', 0.6555202603340149)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "w2v_model.most_similar(vec, topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c6b1b64-c403-437b-81d4-ab286beb247c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c6b1b64-c403-437b-81d4-ab286beb247c",
        "outputId": "e5e2b866-f480-4fae-b9b1-30da79501fe2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7786749005317688),\n",
              " ('queen-mother', 0.7143871784210205),\n",
              " ('king-', 0.6981282234191895),\n",
              " ('queen-consort', 0.6724597811698914),\n",
              " ('monarch', 0.6666999459266663)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "w2v_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce9af9c8-9fb0-4c2b-bd4f-bf95fe9f40ae",
      "metadata": {
        "tags": [],
        "id": "ce9af9c8-9fb0-4c2b-bd4f-bf95fe9f40ae"
      },
      "source": [
        "### Gensim 2 model, saving and reloading test to check if model will work on next tier ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0435f3-2c81-41f1-90cf-e3d0ed8b0647",
      "metadata": {
        "id": "4e0435f3-2c81-41f1-90cf-e3d0ed8b0647"
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# # Store just the words + their trained embeddings.\n",
        "w2v_model.save(\"word2vec1.wordvectors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0765a86f-0657-45e3-b264-8e6ea3bdadb6",
      "metadata": {
        "id": "0765a86f-0657-45e3-b264-8e6ea3bdadb6"
      },
      "outputs": [],
      "source": [
        "# Load back with memory-mapping = read-only, shared across processes.\n",
        "wv = KeyedVectors.load(\"word2vec1.wordvectors\", mmap='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff6146ae-cdf9-490a-8a7a-7d8bb692f40c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff6146ae-cdf9-490a-8a7a-7d8bb692f40c",
        "outputId": "b8ad5214-59a0-4d3f-f034-5d8274105e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.089791   -0.017821   -0.037268   -0.013295   -0.0090625  -0.03825\n",
            " -0.026104   -0.089276    0.051467   -0.0021857  -0.0074708  -0.016283\n",
            " -0.056232    0.026517   -0.040413    0.011108    0.1075      0.05117\n",
            "  0.038878   -0.0071759  -0.027288    0.052936    0.0048894   0.053154\n",
            " -0.012498   -0.0036237  -0.031653    0.0080834  -0.018473    0.048122\n",
            "  0.0098432  -0.014012    0.0041448  -0.039648    0.03404     0.012708\n",
            "  0.02619    -0.010029    0.06783    -0.028244   -0.068825   -0.11998\n",
            " -0.034457    0.014159    0.019905   -0.045616    0.057433   -0.039549\n",
            " -0.002202   -0.0027748  -0.012895    0.01229     0.011554    0.017499\n",
            " -0.054615    0.058455    0.033276    0.019049   -0.03536    -0.034284\n",
            "  0.014129   -0.014776    0.11628     0.057433    0.045229   -0.0088706\n",
            "  0.018039    0.023135   -0.034525    0.024178    0.012592    0.013363\n",
            "  0.032109    0.016538   -0.0022877  -0.027777    0.025676   -0.019492\n",
            "  0.025357   -0.038128   -0.028559   -0.011518   -0.043368    0.037497\n",
            " -0.022898   -0.051914   -0.034729   -0.019702    0.034094    0.022832\n",
            "  0.034344    0.032576   -0.072017   -0.0045942   0.030146    0.076872\n",
            "  0.050725   -0.040963    0.025406    0.040564    0.052325   -0.0090628\n",
            "  0.025757   -0.066618    0.0038035  -0.11481     0.0069563   0.053793\n",
            "  0.0017904  -0.060045   -0.039439    0.075231    0.045516    0.0076306\n",
            "  0.035642    0.0022663  -0.044976   -0.032079   -0.053972    0.0048121\n",
            " -0.0021037   0.048039   -0.011026   -0.0078287   0.03605     0.015382\n",
            " -0.017687   -0.069547   -0.051485    0.073545   -0.029692   -0.0084288\n",
            "  0.0052575   0.008806    0.081943   -0.053992    0.065899   -0.016523\n",
            " -0.009028    0.013822    0.059475    0.087393    0.0073438  -0.0018805\n",
            " -0.039636   -0.011135   -0.035042   -0.06162     0.0096151   0.0085137\n",
            "  0.04132     0.067992    0.02152    -0.065276   -0.0018999  -0.0064729\n",
            " -0.013173    0.023508    0.02984    -0.028208    0.025501    0.0022723\n",
            " -0.038196   -0.031813    0.002195   -0.03152    -0.0065469  -0.032375\n",
            "  0.005915    0.073178    0.0058703  -0.0056682   0.019146    0.024598\n",
            "  0.050551    0.046779   -0.024081    0.052085   -0.021388    0.044073\n",
            "  0.01666    -0.0030246  -0.096832   -0.012947   -0.038147   -0.040804\n",
            " -0.09654     0.077376   -0.01009    -0.020218   -0.010238    0.037737\n",
            "  0.024175   -0.00037449  0.026212   -0.023587   -0.053021    0.028012\n",
            " -0.039966    0.029374   -0.11269     0.041651   -0.040708    0.003206\n",
            " -0.010717    0.024737    0.0077022  -0.044564   -0.0425     -0.020799\n",
            "  0.1206      0.027633    0.0048449   0.0052138  -0.029874   -0.016569\n",
            "  0.0054956   0.012359   -0.0035728   0.009852    0.047506    0.030919\n",
            " -0.024069    0.072115   -0.0024783  -0.05792     0.036772    0.067592\n",
            "  0.0064909  -0.015237    0.0077132  -0.044224   -0.051939   -0.053925\n",
            " -0.0012631  -0.02779     0.045007    0.02117    -0.025188   -0.032471\n",
            "  0.051413   -0.033906    0.06015     0.024762   -0.016328    0.032362\n",
            "  0.015867    0.014273   -0.040641    0.015253    0.022929    0.022105\n",
            " -0.0073612   0.00869     0.001843    0.013384   -0.013135    0.010691\n",
            " -0.059226   -0.038205    0.0072687  -0.016767   -0.017736    0.015479\n",
            " -0.0057892  -0.03502    -0.019997    0.0087257   0.02489     0.020623\n",
            " -0.043068    0.070631   -0.083891   -0.031738   -0.022192   -0.024818\n",
            "  0.033111    0.030611    0.042021   -0.018253    0.086557    0.016666\n",
            "  0.020965   -0.031061   -0.013204   -0.0049876  -0.05896    -0.026331\n",
            " -0.013875    0.03357    -0.10098    -0.00090432 -0.070733    0.0083173\n",
            "  0.034299    0.044272   -0.035651   -0.031911    0.067389   -0.0020026 ]\n"
          ]
        }
      ],
      "source": [
        "### test of the reloaded model\n",
        "vector = wv['computer']  # Get numpy vector of a word\n",
        "print (vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6f433ba-3075-46d5-a94d-4d9184fc69d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6f433ba-3075-46d5-a94d-4d9184fc69d5",
        "outputId": "43219ae4-83c1-48ee-954d-8dd6ef5bafd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('computer', 1.0000001192092896), ('computers', 0.8472651243209839)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "wv.most_similar(vector, topn=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74c3065c-34eb-4f27-8ec8-8d36dbbc5d45",
      "metadata": {
        "id": "74c3065c-34eb-4f27-8ec8-8d36dbbc5d45"
      },
      "outputs": [],
      "source": [
        "#model = w2v_model\n",
        "vec1 = wv['airplane']\n",
        "vec2 = wv['wings']\n",
        "vec3 = wv['boat']\n",
        "vec4 = wv['water']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a702033-7fa5-4736-942c-252a7431c323",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a702033-7fa5-4736-942c-252a7431c323",
        "outputId": "97c606d9-0574-459d-fae1-9844164fb34d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 4.48516011e-02  1.77494735e-02  7.00179907e-03  2.13823602e-01\n",
            "  1.42352998e-01  9.28856954e-02 -2.20099986e-02 -8.93699974e-02\n",
            "  1.00948006e-01  2.01800186e-03 -4.16420028e-02 -1.61290038e-02\n",
            "  6.18999917e-03 -2.25130022e-02  4.42000031e-02  3.72980013e-02\n",
            "  9.74790007e-02  4.33389992e-02 -2.26499960e-02  2.24609990e-02\n",
            "  1.98272001e-02 -5.42418584e-02  4.16069999e-02  1.57020055e-02\n",
            "  5.17360009e-02 -7.53870010e-02 -6.99819950e-03 -1.45260012e-02\n",
            "  6.85199909e-03  1.14156596e-01 -3.56000103e-03  1.62874997e-01\n",
            "  1.69504017e-01 -1.26860011e-02  6.13770001e-02 -3.24536003e-02\n",
            "  6.20739982e-02 -5.01598008e-02 -1.03100576e-03 -5.56529015e-02\n",
            "  3.53326052e-02 -1.79419994e-01 -3.04399990e-02  9.51298745e-04\n",
            "  8.08179975e-02 -6.09169975e-02 -2.53149942e-02 -1.24399969e-03\n",
            " -5.65499999e-02  1.04581006e-02  8.44020024e-02 -1.09930001e-02\n",
            "  7.96940029e-02 -8.91719982e-02 -3.00120041e-02  3.85790020e-02\n",
            " -1.54500082e-03 -1.60820037e-02  3.93669978e-02  7.51660019e-03\n",
            "  8.26187953e-02  4.22400013e-02 -1.54800043e-02  1.53690055e-02\n",
            "  1.62059981e-02  8.81119967e-02  6.05482049e-02 -6.32819980e-02\n",
            "  4.03020009e-02  1.28789991e-02  1.81071997e-01  5.46022989e-02\n",
            "  1.71918012e-02  1.63225997e-02 -3.19847986e-02 -2.42677983e-02\n",
            " -2.62960000e-03  5.76281920e-02 -2.44730003e-02  3.08720004e-02\n",
            "  3.06800008e-03 -8.68109986e-02  1.04204994e-02  1.63739957e-02\n",
            " -3.06222010e-02  7.68269971e-02  9.57629979e-02 -5.42460009e-02\n",
            "  8.71730968e-02 -6.44831061e-02  7.73449987e-02  1.08290002e-01\n",
            " -1.00699998e-01 -1.82324331e-02 -9.15979967e-02  1.66110024e-02\n",
            "  1.89219005e-02  2.09010001e-02  8.56494009e-02 -8.15199986e-02\n",
            " -6.98637962e-02 -1.75958008e-01  4.23683040e-02 -9.10015032e-02\n",
            " -2.05155946e-02 -1.41093001e-01  1.57490000e-02  1.78917706e-01\n",
            " -2.00079940e-03 -4.27089967e-02  1.49820998e-01  1.89918011e-01\n",
            " -1.16080046e-02  3.28280069e-02  3.15180011e-02 -6.62570000e-02\n",
            " -4.20239009e-02 -3.72489989e-02 -3.81009988e-02 -6.17460012e-02\n",
            " -3.09740007e-02  4.33079973e-02  3.02969962e-02 -2.51781121e-02\n",
            "  9.64099914e-03  4.40683961e-02 -4.58619967e-02 -1.09176993e-01\n",
            " -1.89887792e-01 -5.08820042e-02  2.59140003e-02 -1.43309996e-01\n",
            " -1.56997010e-01 -9.20141041e-02  9.97640043e-02 -2.70569976e-02\n",
            "  1.40267998e-01 -4.33813818e-02  2.89511010e-02  1.40753999e-01\n",
            " -2.72652991e-02 -6.59520030e-02 -8.76629949e-02  3.76399979e-03\n",
            " -2.15869993e-02  1.15299970e-03 -4.91239987e-02 -7.28380084e-02\n",
            "  6.86200038e-02 -4.82600033e-02  2.85199974e-02 -5.77099621e-03\n",
            " -5.29749952e-02 -1.16200000e-03 -3.52199972e-02 -2.40201000e-02\n",
            " -1.99480057e-02 -2.86810026e-02 -4.97110039e-02 -3.65720019e-02\n",
            "  5.31750023e-02 -3.85049991e-02 -1.71609998e-01  4.80252951e-02\n",
            "  8.31900164e-03 -1.84919983e-02  8.21326077e-02 -8.07316080e-02\n",
            "  1.65165007e-01  8.76950026e-02  7.38329999e-03  1.28006995e-01\n",
            " -1.81999058e-04  3.07279974e-02  8.20241943e-02  5.98830022e-02\n",
            " -1.12486996e-01  1.64507002e-01 -5.71129993e-02  6.82699978e-02\n",
            " -1.79299712e-03 -1.59940049e-02  1.37279987e-01 -1.66557014e-01\n",
            "  3.87530029e-02 -1.16195902e-01 -9.65211987e-02  1.55409008e-01\n",
            "  6.64120018e-02 -8.96380022e-02 -4.11969945e-02 -3.42923030e-02\n",
            "  1.17754199e-01  3.75015028e-02  5.24757951e-02 -5.86169958e-03\n",
            " -2.59769969e-02  4.58519980e-02 -7.82250017e-02  4.73999977e-03\n",
            " -1.24570005e-01  7.55290017e-02 -2.56009996e-02 -9.85860080e-02\n",
            "  1.47329003e-01  1.46500021e-02 -1.14163995e-01  1.36949001e-02\n",
            " -8.20819959e-02  7.02000037e-03  4.05000001e-02 -5.48337102e-02\n",
            " -7.89045021e-02  1.70244984e-02  2.40158979e-02  2.49070004e-02\n",
            "  1.02644995e-01 -1.26997992e-01  6.91362992e-02 -7.82169998e-02\n",
            " -7.79879931e-03 -6.14750013e-02 -9.00911987e-02  6.68529943e-02\n",
            " -1.58958018e-01  1.68641001e-01  2.23690011e-02  5.87622002e-02\n",
            " -7.52120987e-02 -5.49314991e-02 -1.75411981e-02 -4.96700034e-02\n",
            " -1.02209896e-02 -3.27551998e-02 -1.41724013e-02  1.91386998e-01\n",
            " -9.75461006e-02  7.56990016e-02  1.10307097e-01 -1.51321203e-01\n",
            "  6.25119954e-02  8.01789984e-02  1.14767998e-01 -5.79580031e-02\n",
            " -5.24099991e-02  1.40079930e-02  8.71720016e-02  2.16859970e-02\n",
            " -1.43761992e-01  8.11270028e-02  6.71639945e-03 -5.86599968e-02\n",
            "  2.82879975e-02  1.21090021e-02 -2.25169910e-03 -5.37789986e-02\n",
            " -1.33810169e-03  8.53645951e-02 -1.59929991e-02  7.55349994e-02\n",
            " -7.31830001e-02  4.64119017e-02 -8.88603032e-02  7.20239952e-02\n",
            " -2.26651020e-02 -1.44019991e-01 -6.75729960e-02 -2.46017985e-02\n",
            " -5.49530014e-02  1.22774534e-01 -5.06138988e-02  2.09340081e-02\n",
            " -1.24006994e-01 -3.76849994e-02 -7.33139962e-02 -6.00470006e-02\n",
            " -1.02124706e-01  9.24900081e-03  1.53225005e-01 -7.73100182e-03\n",
            "  1.60408989e-01 -7.81929940e-02 -1.17609099e-01  6.80923015e-02\n",
            " -1.95428990e-02  1.30347610e-01 -8.07109028e-02  1.54660009e-02\n",
            "  6.20779991e-02 -2.79900022e-02 -5.45770023e-03  5.72469980e-02\n",
            " -5.85740060e-02  4.03900072e-03 -2.45209970e-02  5.90009987e-02\n",
            "  9.02819112e-02 -1.32027194e-01 -2.23400015e-02 -4.00919700e-03]\n"
          ]
        }
      ],
      "source": [
        "result_vec = vec1 - vec2 + vec4\n",
        "print(result_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1692e122-b612-4a92-b7f2-4eb604a59b54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1692e122-b612-4a92-b7f2-4eb604a59b54",
        "outputId": "d53022bd-fe34-47b1-89e7-c53e0f58c00b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('water', 0.6646305322647095),\n",
              " ('water--', 0.5573056936264038),\n",
              " ('drinking-water', 0.5420796871185303),\n",
              " ('waste-water', 0.5359698534011841),\n",
              " ('potable', 0.5343053936958313)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "wv.similar_by_vector(result_vec, topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bab826a-d80d-4f31-8008-be369b0932fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bab826a-d80d-4f31-8008-be369b0932fc",
        "outputId": "6997f154-9710-4296-e55e-c7ebb00da1ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('water', 0.6646305322647095),\n",
              " ('water--', 0.5573056936264038),\n",
              " ('drinking-water', 0.5420796871185303),\n",
              " ('waste-water', 0.5359698534011841),\n",
              " ('potable', 0.5343053936958313)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "wv.most_similar(result_vec, topn=5)  ### same output as similar_by_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83264a0b-83c2-4a58-9b47-1b38117ce79d",
      "metadata": {
        "tags": [],
        "id": "83264a0b-83c2-4a58-9b47-1b38117ce79d"
      },
      "source": [
        "### 2. Average word2vec of a given string  ###\n",
        "\n",
        "    Download Poem_sentiment and create a function to calculate the average word2vec of a given string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b01954f-06b6-4103-baef-3af1098700a7",
      "metadata": {
        "id": "0b01954f-06b6-4103-baef-3af1098700a7"
      },
      "outputs": [],
      "source": [
        "### This function can take a sentence and split it for processing\n",
        "def take_sentence_and_split_into_array(sentence):\n",
        "  \"\"\"\n",
        "  Reads a sentence from the user and splits it into an array.\n",
        "\n",
        "  Returns:\n",
        "    The sentence, stored in an array.\n",
        "  \"\"\"\n",
        "  print(sentence)\n",
        "  words = sentence.split(\" \")\n",
        "  array = []\n",
        "\n",
        "  for word in words:\n",
        "    array.append(word)\n",
        "\n",
        "  return array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27178458-a83c-48ae-b2a4-fdc7ddc202b7",
      "metadata": {
        "id": "27178458-a83c-48ae-b2a4-fdc7ddc202b7"
      },
      "outputs": [],
      "source": [
        "### This function allow the user to input their own sentence for processing\n",
        "def read_sentence_and_split_into_array():\n",
        "  \"\"\"\n",
        "  Reads a sentence from the user and splits it into an array.\n",
        "\n",
        "  Returns:\n",
        "    The sentence, stored in an array.\n",
        "  \"\"\"\n",
        "  sentence = input(\"Enter a sentence: \")\n",
        "  words = sentence.split(\" \")\n",
        "  array = []\n",
        "\n",
        "  for word in words:\n",
        "    array.append(word)\n",
        "\n",
        "  return array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a10c38d1-b418-424f-96de-6ae72cab63b5",
      "metadata": {
        "id": "a10c38d1-b418-424f-96de-6ae72cab63b5"
      },
      "outputs": [],
      "source": [
        "# This function calculates the average word2vec of a text array\n",
        "import numpy as np\n",
        "#from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def average_word2vec(text_list):\n",
        "    model = w2v_model   ### hardcode of model to be used from setup\n",
        "    test = []\n",
        "\n",
        "##loop the given sentence\n",
        "    for word in text_list:\n",
        "        try:\n",
        "            word_embeding = model.get_vector(word, norm=True)\n",
        "            test.append(np.average(word_embeding,axis=0))\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    return test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73cb79d8-cab4-4323-8b12-ce4ad8cfda25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73cb79d8-cab4-4323-8b12-ce4ad8cfda25",
        "outputId": "35339843-c1da-4518-ecc5-d05ce9eb1ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "she sought for flowers\n",
            "['she', 'sought', 'for', 'flowers']\n",
            "number of words in sentence 4\n"
          ]
        }
      ],
      "source": [
        "### take a random sentence from the test data set loaded from poem_sentiment\n",
        "sentence = random.choice(X_test)\n",
        "\n",
        "sentence_array = take_sentence_and_split_into_array(sentence)\n",
        "print(sentence_array)\n",
        "print('number of words in sentence', len(sentence_array))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbbb4f91-7c8f-48ee-a084-ae4816fa7ede",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbbb4f91-7c8f-48ee-a084-ae4816fa7ede",
        "outputId": "414334dc-1122-42e1-cc32-e7e7b10ca13c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average word2vec of the string [-0.0034214917, -0.0009899102, -0.002198077, 7.521182e-05]\n"
          ]
        }
      ],
      "source": [
        "### Print the average word2vec of input sentence\n",
        "avg_word2vec = average_word2vec(sentence_array)\n",
        "print ('Average word2vec of the string', avg_word2vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b54f43-bd9e-47e1-a967-51a5d2bf5e25",
      "metadata": {
        "tags": [],
        "id": "c1b54f43-bd9e-47e1-a967-51a5d2bf5e25"
      },
      "source": [
        "### Input your own phrase ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3483ba3a-8fa0-4dc5-b012-2b847584090f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3483ba3a-8fa0-4dc5-b012-2b847584090f",
        "outputId": "60883bc3-cc72-4629-ea45-f1291f401558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: When in the course of human events\n",
            "['When', 'in', 'the', 'course', 'of', 'human', 'events']\n",
            "7\n"
          ]
        }
      ],
      "source": [
        "sentence_array = read_sentence_and_split_into_array()\n",
        "print(sentence_array)\n",
        "print(len(sentence_array))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ef6f6e7-5cab-453c-9943-49b410496b4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ef6f6e7-5cab-453c-9943-49b410496b4a",
        "outputId": "5b80d750-038b-495f-9e81-913bf2801103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['When', 'in', 'the', 'course', 'of', 'human', 'events']\n",
            "number of words in sentence 7\n"
          ]
        }
      ],
      "source": [
        "print(sentence_array)\n",
        "print('number of words in sentence', len(sentence_array))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50759ab8-847e-4a1a-8171-ae4aaa31a3e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50759ab8-847e-4a1a-8171-ae4aaa31a3e8",
        "outputId": "1e455c5e-27b7-42de-d4ff-c2b29a870382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00048712676, -0.004368693, 0.00017166903, 0.0017402467, 0.0005325078, 0.00057192304, 0.0060858387]\n"
          ]
        }
      ],
      "source": [
        "avg_word2vec = average_word2vec(sentence_array)\n",
        "print (avg_word2vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cosine similarity** is a measure of similarity between two non-zero vectors of an inner product space based on the cosine of the angle between them, resulting in a value between -1 and 1. A cosine similarity of 1 indicates that the two vectors are pointing in the same direction, while a cosine similarity of -1 indicates that they are pointing in opposite directions. A cosine similarity of 0 indicates that the two vectors are orthogonal.\n",
        "\n",
        "The cosine similarity function is often used in natural language processing to measure the similarity between two documents, or between a document and a query. It can also be used to measure the similarity between two images."
      ],
      "metadata": {
        "id": "D0wMqNSYNHDY"
      },
      "id": "D0wMqNSYNHDY"
    },
    {
      "cell_type": "code",
      "source": [
        "### nltk - Natural Language ToolKit\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkBJrxhPiiLD",
        "outputId": "5828f651-81f3-43a6-8f59-6b06b0130b51"
      },
      "id": "LkBJrxhPiiLD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBpRDcdUD4cI",
        "outputId": "fa0f454a-20d9-437e-d10e-0147c60f0bf9"
      },
      "id": "MBpRDcdUD4cI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=c90efd8a34e4ac78cb441023d719e77d17312e2035541c584d99a71ec434c517\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, transformers, sentence-transformers\n",
            "Successfully installed safetensors-0.3.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Tokenize the text input.\n",
        "text = sentence\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Create a vector representation for each token.\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(tokens)\n",
        "\n",
        "# Calculate the cosine similarity between the text input and each phrase in the poem dataset.\n",
        "phrases = X_train\n",
        "similarities = []\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "for phrase in phrases:\n",
        "    sentence_1= model.encode(sentence, convert_to_tensor=True)\n",
        "    sentence_2 = model.encode(phrase, convert_to_tensor=True)\n",
        "    similarity = util.pytorch_cos_sim(sentence_1, sentence_2)\n",
        "    similarities.append(similarity)\n",
        "\n",
        "max(similarities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9KTFwraf0qL",
        "outputId": "c5b6c4b3-cddb-43d4-91c3-9c78656108a7"
      },
      "id": "a9KTFwraf0qL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5086]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Max cosine Similarity ', max(similarities))\n",
        "print ('Closest phrase:  ', X_train[similarities.index(max(similarities))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJFHUhuC_4cE",
        "outputId": "745c5fc4-b60e-410a-eea8-83d1b26edc3f"
      },
      "id": "cJFHUhuC_4cE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max cosine Similarity  tensor([[0.5086]])\n",
            "Closest phrase:   make a fragrance of her fame.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (X_train[similarities.index(max(similarities))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF2y8p5BEqnX",
        "outputId": "d590fa41-a9df-41d4-8238-a2f72d4bdedb"
      },
      "id": "XF2y8p5BEqnX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make a fragrance of her fame.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ce9af9c8-9fb0-4c2b-bd4f-bf95fe9f40ae"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}